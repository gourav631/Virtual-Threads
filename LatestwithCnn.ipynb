{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-04-22T21:44:46.198809Z",
     "iopub.status.busy": "2023-04-22T21:44:46.198501Z",
     "iopub.status.idle": "2023-04-22T21:44:54.852544Z",
     "shell.execute_reply": "2023-04-22T21:44:54.851397Z",
     "shell.execute_reply.started": "2023-04-22T21:44:46.198780Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-22T21:45:02.073456Z",
     "iopub.status.busy": "2023-04-22T21:45:02.072790Z",
     "iopub.status.idle": "2023-04-22T22:04:10.866115Z",
     "shell.execute_reply": "2023-04-22T22:04:10.864870Z",
     "shell.execute_reply.started": "2023-04-22T21:45:02.073416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7508 images belonging to 4 classes.\n",
      "Epoch 1/10\n",
      "235/235 [==============================] - 141s 534ms/step - loss: 0.4631 - accuracy: 0.8347\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 101s 431ms/step - loss: 0.2204 - accuracy: 0.9233\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 99s 419ms/step - loss: 0.1475 - accuracy: 0.9487\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 98s 418ms/step - loss: 0.1203 - accuracy: 0.9575\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 105s 446ms/step - loss: 0.0843 - accuracy: 0.9724\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 101s 430ms/step - loss: 0.0763 - accuracy: 0.9739\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 99s 423ms/step - loss: 0.0565 - accuracy: 0.9823\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 99s 421ms/step - loss: 0.0526 - accuracy: 0.9830\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 98s 418ms/step - loss: 0.0406 - accuracy: 0.9872\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 100s 428ms/step - loss: 0.0421 - accuracy: 0.9871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x78f640209d50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Load the dataset using Keras ImageDataGenerator\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '/kaggle/input/imgess/images',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-22T22:04:43.878243Z",
     "iopub.status.busy": "2023-04-22T22:04:43.877244Z",
     "iopub.status.idle": "2023-04-22T22:04:44.544566Z",
     "shell.execute_reply": "2023-04-22T22:04:44.542995Z",
     "shell.execute_reply.started": "2023-04-22T22:04:43.878196Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "# model.save('/kaggle/working/clothing_modelCnnV1.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-22T22:16:01.747120Z",
     "iopub.status.busy": "2023-04-22T22:16:01.746462Z",
     "iopub.status.idle": "2023-04-22T22:16:02.176388Z",
     "shell.execute_reply": "2023-04-22T22:16:02.175307Z",
     "shell.execute_reply.started": "2023-04-22T22:16:01.747079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 335ms/step\n",
      "The predicted class is: shirts\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "img = image.load_img('/kaggle/input/testimg1/clothing_image.jpg', target_size=(224, 224))\n",
    "\n",
    "# Convert the image to a numpy array\n",
    "img = np.array(img)\n",
    "\n",
    "# Normalize the pixel values\n",
    "img = img.astype('float32') / 255.0\n",
    "\n",
    "# Add an extra dimension to the array to represent batch size\n",
    "img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
    "\n",
    "# Make a prediction on the image\n",
    "probabilities = model.predict(img)\n",
    "\n",
    "# Get the class label with the highest probability\n",
    "class_index = np.argmax(probabilities)\n",
    "\n",
    "# Get the corresponding class name\n",
    "class_names = ['pants', 'shirts', 'shorts', 'tshirt']\n",
    "class_name = class_names[class_index]\n",
    "\n",
    "print('The predicted class is:', class_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import time\n",
    "holistics = mp.solutions.holistic # To bring our holistic model\n",
    "drawing = mp.solutions.drawing_utils # Use fot drawing the utilities\n",
    "\n",
    "def Detection(img, model):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    img.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(img)                 # Make prediction\n",
    "    img.flags.writeable = True                   # Image is now writeable \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return img, results\n",
    "def DrawingCustomLandmarks(image, results):\n",
    "    # Draw face connections\n",
    "    drawing.draw_landmarks(image, results.face_landmarks, holistics.FACEMESH_TESSELATION, \n",
    "                             drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    drawing.draw_landmarks(image, results.pose_landmarks, holistics.POSE_CONNECTIONS,\n",
    "                             drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    drawing.draw_landmarks(image, results.left_hand_landmarks, holistics.HAND_CONNECTIONS, \n",
    "                             drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    drawing.draw_landmarks(image, results.right_hand_landmarks, holistics.HAND_CONNECTIONS, \n",
    "                             drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    \n",
    "    \n",
    "    \n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with holistics.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = Detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        DrawingCustomLandmarks(image, results)\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow('Pose Estimation Window', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load the YOLOv4-tiny object detection model\n",
    "net = cv2.dnn.readNetFromDarknet(\"yolov4-tiny.cfg\", \"yolov4-tiny.weights\")\n",
    "\n",
    "# Define the classes that the model can detect\n",
    "classes = [\"shirt\", \"pants\", \"tshirt\", \"shorts\"]\n",
    "\n",
    "# Initialize the video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read frames from the video capture\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Perform object detection on the frame\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416,416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i - 1] for i in [net.getUnconnectedOutLayers()][0]]\n",
    "    outputs = net.forward(output_layers)\n",
    "\n",
    "    # Process the output of the object detection model\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for output in outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * frame.shape[1])\n",
    "                center_y = int(detection[1] * frame.shape[0])\n",
    "                width = int(detection[2] * frame.shape[1])\n",
    "                height = int(detection[3] * frame.shape[0])\n",
    "                left = int(center_x - width/2)\n",
    "                top = int(center_y - height/2)\n",
    "                class_ids.append(class_id)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([left, top, width, height])\n",
    "\n",
    "    # Draw bounding boxes around the detected objects\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    if len(indices) > 0:\n",
    "        for i in indices.flatten():\n",
    "            left, top, width, height = boxes[i]\n",
    "            label = classes[class_ids[i]]\n",
    "            confidence = confidences[i]\n",
    "            cv2.rectangle(frame, (left, top), (left + width, top + height), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{label} {confidence:.2f}\", (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow(\"Object Detection\", frame)\n",
    "\n",
    "    # Check for key presses and exit gracefully if necessary\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining the object detection and pose estimation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "model_path = 'yolov4-tiny.weights'\n",
    "config_path = 'yolov4-tiny.cfg'\n",
    "net = cv2.dnn.readNetFromCaffe(config_path, model_path)\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "cap = cv2.VideoCapture(0)\n",
    "ret, frame = cap.read()\n",
    "blob = cv2.dnn.blobFromImage(frame, 0.007843, (frame.shape[1], frame.shape[0]), (127.5, 127.5, 127.5), False)\n",
    "net.setInput(blob)\n",
    "detections = net.forward()\n",
    "for i in range(detections.shape[2]):\n",
    "    confidence = detections[0, 0, i, 2]\n",
    "    if confidence > 0.5:\n",
    "        class_id = int(detections[0, 0, i, 1])\n",
    "        if class_id == 1: # Shirt\n",
    "            # Perform pose estimation on the upper body\n",
    "            # (you may need to adjust the coordinates based on the output of your object detection model)\n",
    "            upper_body = frame[y:y+h, x:x+w]\n",
    "            results = pose.process(cv2.cvtColor(upper_body, cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "            # Get the pose landmarks and estimate the torso angle\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            torso_angle = # Estimate the torso angle using the pose landmarks\n",
    "            \n",
    "            # Fit the shirt to the torso using the estimated angle\n",
    "            # (you may need to adjust the coordinates and angle based on your shirt and torso models)\n",
    "            shirt = # Load the shirt model\n",
    "            shirt.fit_to_torso(upper_body, landmarks, torso_angle)\n",
    "cv2.imshow('Frame', frame)\n",
    "cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "71\n",
      "70\n",
      "73\n",
      "75\n",
      "75\n",
      "74\n",
      "74\n",
      "74\n",
      "74\n",
      "74\n",
      "74\n",
      "74\n",
      "75\n",
      "75\n",
      "75\n",
      "74\n",
      "74\n",
      "74\n",
      "74\n",
      "74\n",
      "75\n",
      "77\n",
      "75\n",
      "77\n",
      "78\n",
      "78\n",
      "79\n",
      "77\n",
      "78\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import cvzone\n",
    "import cv2\n",
    "from cvzone.PoseModule import PoseDetector\n",
    "\n",
    "# cap = cv2.VideoCapture(\"Resources/Videos/1.mp4\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = PoseDetector()\n",
    "cap.set(3, 1280)  # Set the width of the frame to 1280\n",
    "cap.set(4, 720)   # Set the height of the frame to 720\n",
    "\n",
    "\n",
    "shirtFolderPath = \"Resources/Shirts\"\n",
    "listShirts = os.listdir(shirtFolderPath)\n",
    "# print(listShirts)\n",
    "fixedRatio = 262 / 190  \n",
    "shirtRatioHeightWidth = 581 / 440\n",
    "imageNumber = 0\n",
    "imgButtonRight = cv2.imread(\"Resources/button.png\", cv2.IMREAD_UNCHANGED)\n",
    "imgButtonLeft = cv2.flip(imgButtonRight, 1)\n",
    "counterRight = 0\n",
    "counterLeft = 0\n",
    "selectionSpeed = 10\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    img = detector.findPose(img)\n",
    "    # img = cv2.flip(img,1)\n",
    "    lmList, bboxInfo = detector.findPosition(img, bboxWithHands=False, draw=False)\n",
    "    if lmList:\n",
    "        # center = bboxInfo[\"center\"]\n",
    "        lm11 = lmList[11][1:3]\n",
    "        lm12 = lmList[12][1:3]\n",
    "        imgShirt = cv2.imread(os.path.join(shirtFolderPath, listShirts[imageNumber]), cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "        widthOfShirt = int((lm11[0] - lm12[0]) * fixedRatio)\n",
    "        print(widthOfShirt)\n",
    "        imgShirt = cv2.resize(imgShirt, (widthOfShirt, int(widthOfShirt * shirtRatioHeightWidth)))\n",
    "        currentScale = (lm11[0] - lm12[0]) / 190\n",
    "        offset = int(44 * currentScale), int(48 * currentScale)\n",
    "\n",
    "        try:\n",
    "            img = cvzone.overlayPNG(img, imgShirt, (lm12[0] - offset[0], lm12[1] - offset[1]))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "#         img = cvzone.overlayPNG(img, imgButtonRight, (1074, 293))\n",
    "#         img = cvzone.overlayPNG(img, imgButtonLeft, (72, 293))\n",
    "\n",
    "#         if lmList[16][1] < 300:\n",
    "#             counterRight += 1\n",
    "#             cv2.ellipse(img, (139, 360), (66, 66), 0, 0,\n",
    "#                         counterRight * selectionSpeed, (0, 255, 0), 20)\n",
    "#             if counterRight * selectionSpeed > 360:\n",
    "#                 counterRight = 0\n",
    "#                 if imageNumber < len(listShirts) - 1:\n",
    "#                     imageNumber += 1\n",
    "#         elif lmList[15][1] > 900:\n",
    "#             counterLeft += 1\n",
    "#             cv2.ellipse(img, (1138, 360), (66, 66), 0, 0,\n",
    "#                         counterLeft * selectionSpeed, (0, 255, 0), 20)\n",
    "#             if counterLeft * selectionSpeed > 360:\n",
    "#                 counterLeft = 0\n",
    "#                 if imageNumber > 0:\n",
    "#                     imageNumber -= 1\n",
    "\n",
    "#         else:\n",
    "#             counterRight = 0\n",
    "#             counterLeft = 0\n",
    "\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run Clothapp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445\n",
      "438\n",
      "431\n",
      "420\n",
      "415\n",
      "417\n",
      "420\n",
      "421\n",
      "421\n",
      "424\n",
      "428\n",
      "430\n",
      "435\n",
      "439\n",
      "435\n",
      "434\n",
      "434\n",
      "435\n",
      "437\n",
      "437\n",
      "437\n",
      "435\n",
      "437\n",
      "437\n",
      "435\n",
      "435\n",
      "432\n",
      "432\n",
      "434\n",
      "434\n",
      "434\n",
      "434\n",
      "434\n",
      "437\n",
      "439\n",
      "441\n",
      "442\n",
      "442\n",
      "442\n",
      "441\n",
      "439\n",
      "441\n",
      "442\n",
      "442\n",
      "445\n",
      "445\n",
      "448\n",
      "449\n",
      "449\n",
      "449\n",
      "446\n",
      "445\n",
      "445\n",
      "444\n",
      "442\n",
      "441\n",
      "439\n",
      "437\n",
      "437\n",
      "437\n",
      "437\n",
      "439\n",
      "439\n",
      "439\n",
      "439\n",
      "441\n",
      "438\n",
      "438\n",
      "438\n",
      "437\n",
      "439\n",
      "441\n",
      "420\n",
      "401\n",
      "405\n",
      "406\n",
      "408\n",
      "408\n",
      "408\n",
      "409\n",
      "427\n",
      "432\n",
      "432\n",
      "435\n",
      "435\n",
      "437\n",
      "437\n",
      "439\n",
      "441\n",
      "441\n",
      "442\n",
      "442\n",
      "441\n",
      "442\n",
      "442\n",
      "442\n",
      "445\n",
      "445\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import cvzone\n",
    "import cv2\n",
    "from cvzone.PoseModule import PoseDetector\n",
    "\n",
    "# cap = cv2.VideoCapture(\"Resources/Videos/1.mp4\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = PoseDetector()\n",
    "cap.set(3, 1280)  # Set the width of the frame to 1280\n",
    "cap.set(4, 720)   # Set the height of the frame to 720\n",
    "\n",
    "\n",
    "shirtFolderPath = \"Resources/Shirts\"\n",
    "listShirts = os.listdir(shirtFolderPath)\n",
    "# print(listShirts)\n",
    "fixedRatio = 262 / 190  \n",
    "shirtRatioHeightWidth = 581 / 440\n",
    "imageNumber = 0\n",
    "counterRight = 0\n",
    "counterLeft = 0\n",
    "selectionSpeed = 10\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    img = detector.findPose(img)\n",
    "    # img = cv2.flip(img,1)\n",
    "    lmList, bboxInfo = detector.findPosition(img, bboxWithHands=False, draw=False)\n",
    "    if lmList:\n",
    "        # center = bboxInfo[\"center\"]\n",
    "        lm11 = lmList[11][1:3]\n",
    "        lm12 = lmList[12][1:3]\n",
    "        imgShirt = cv2.imread(os.path.join(shirtFolderPath, listShirts[imageNumber]), cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "        widthOfShirt = int((lm11[0] - lm12[0]) * fixedRatio)\n",
    "        print(widthOfShirt)\n",
    "        imgShirt = cv2.resize(imgShirt, (widthOfShirt, int(widthOfShirt * shirtRatioHeightWidth)))\n",
    "        currentScale = (lm11[0] - lm12[0]) / 190\n",
    "        offset = int(44 * currentScale), int(48 * currentScale)\n",
    "\n",
    "        try:\n",
    "            img = cvzone.overlayPNG(img, imgShirt, (lm12[0] - offset[0], lm12[1] - offset[1]))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
